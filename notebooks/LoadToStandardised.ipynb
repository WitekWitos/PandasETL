{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasetl.config import Config\n",
    "from datetime import datetime\n",
    "from pandasetl.entities.categories import ExtractCategories\n",
    "from pandasetl.entities.customer import ExtractCustomer\n",
    "from pandasetl.entities.employee_territories import ExtractEmployee_territories\n",
    "from pandasetl.entities.employees import ExtractEmployees\n",
    "from pandasetl.entities.order_details import ExtractOrder_details\n",
    "from pandasetl.entities.orders import ExtractOrders\n",
    "from pandasetl.entities.products import ExtractProducts\n",
    "from pandasetl.entities.regions import ExtractRegions\n",
    "from pandasetl.entities.shippers import ExtractShippers\n",
    "from pandasetl.entities.suppliers import ExtractSuppliers\n",
    "from pandasetl.entities.territories import ExtractTerritories\n",
    "from pandasetl.pipelines.categories import PipelineCategories\n",
    "from pandasetl.pipelines.customer import PipelineCustomer\n",
    "from pandasetl.pipelines.employee_territories import PipelineEmployee_territories\n",
    "from pandasetl.pipelines.employees import PipelineEmployees\n",
    "from pandasetl.pipelines.order_details import PipelineOrder_details\n",
    "from pandasetl.pipelines.orders import PipelineOrders\n",
    "from pandasetl.pipelines.products import PipelineProducts\n",
    "from pandasetl.pipelines.regions import PipelineRegions\n",
    "from pandasetl.pipelines.shippers import PipelineShippers\n",
    "from pandasetl.pipelines.suppliers import PipelineSuppliers\n",
    "from pandasetl.pipelines.territories import PipelineTerritories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'object_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m load_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m      4\u001b[0m categories_config \u001b[38;5;241m=\u001b[39m Config(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVSC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPandasETL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfilesystem_config.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mobject_name\u001b[49m\n\u001b[0;32m      6\u001b[0m path_dropzone \u001b[38;5;241m=\u001b[39m categories_config\u001b[38;5;241m.\u001b[39mget_dropzone_path(file)\n\u001b[0;32m      7\u001b[0m file_name \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m load_date\n",
      "\u001b[1;31mNameError\u001b[0m: name 'object_name' is not defined"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "status = True\n",
    "categories_config = Config(r'C:\\Users\\1\\VSC\\PandasETL\\config\\filesystem_config.json')\n",
    "file = object_name\n",
    "path_dropzone = categories_config.get_dropzone_path(file)\n",
    "file_name = file + '_' + load_date\n",
    "path_raw = categories_config.get_raw_path(file_name)\n",
    "path_standardised = categories_config.get_standardised_path(file)\n",
    "print('-------------------------')\n",
    "print('load_date: ', load_date) \n",
    "print('file_name: ', file_name) \n",
    "print('path_dropzone: ', path_dropzone) \n",
    "print('path_raw: ', path_raw)\n",
    "print('path_standardised: ', path_standardised)\n",
    "print('-------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading from dropzone to raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_dropzone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_dropzone \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mpath_dropzone\u001b[49m)\n\u001b[0;32m      2\u001b[0m df_dropzone\u001b[38;5;241m.\u001b[39mto_csv(path_raw)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_dropzone' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_dropzone = pd.read_csv(path_dropzone)\n",
    "except FileNotFoundError:\n",
    "    status = False\n",
    "    print(\"Wrong file or file path\")\n",
    "    \n",
    "if status:    \n",
    "    df_dropzone.to_csv(path_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading from raw to standardised <br>\n",
    "####   Implementing raw file, exctracting and changing data in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mpath_raw\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df_raw\u001b[38;5;241m.\u001b[39mitertuples():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_raw' is not defined"
     ]
    }
   ],
   "source": [
    "if status:\n",
    "    df_raw = pd.read_csv(path_raw)\n",
    "\n",
    "    if file == 'categories':\n",
    "        pc = PipelineCategories()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_categories_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'customer':\n",
    "        pc = PipelineCustomer()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_customer_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'employee_territories':\n",
    "        pc = PipelineEmployee_territories()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_employee_territories_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "                                  \n",
    "        \n",
    "    elif file == 'employees':\n",
    "        pc = PipelineEmployees()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_employees_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'order_details':\n",
    "        pc = PipelineOrder_details()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_order_details_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'orders':\n",
    "        pc = PipelineOrders()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_orders_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'products':\n",
    "        pc = PipelineProducts()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_products_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'regions':\n",
    "        pc = PipelineRegions()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_region_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'shippers':\n",
    "        pc = PipelineShippers()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_shippers_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    elif file == 'suppliers':\n",
    "        pc = PipelineSuppliers()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_suppliers_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "       \n",
    "    elif file == 'territories':\n",
    "        pc = PipelineTerritories()\n",
    "        df_transformed = pc.executepipeline(df_raw, result)\n",
    "        partition_col = Config.get_territories_partition_col()\n",
    "        df_transformed[\"loadDate\"] = load_date\n",
    "        df_transformed.to_parquet(path_standardised, partition_cols = partition_col)\n",
    "\n",
    "    else:\n",
    "        print('File is not implemented')\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('No file detected in the dropzone for object: ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf745b7a5c71c7a30f0769585f909cf40a0a8ad718c406df3a9be45d79e164d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
